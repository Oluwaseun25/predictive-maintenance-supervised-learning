{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59c1efa7-2f53-44c8-bac5-1fa5fabe3009",
   "metadata": {},
   "source": [
    "## Project Description:\n",
    "\n",
    "This notebook focuses on predictive maintenance in industrial settings using machine learning techniques. The objective is to develop a binary classification model capable of predicting machine failures. Leveraging various features such as temperature, rotational speed, torque, and tool wear, the model aims to accurately classify whether a machine is likely to experience failure or not. By employing advanced algorithms and robust evaluation methodologies, the notebook seeks to provide actionable insights for proactive maintenance strategies, ultimately enhancing operational efficiency and reducing downtime in industrial processes.\n",
    "\n",
    "## Project Objective:\n",
    "\n",
    "The primary goal of this notebook is to develop a robust binary classification model for predictive maintenance. The model will leverage historical data on machine performance and failure events to predict whether a machine is likely to fail in the future.\n",
    "\n",
    "Data Preprocessing: Prepare the dataset by cleaning, transforming, and encoding features to ensure compatibility with machine learning algorithms.\n",
    "\n",
    "Feature Engineering: Extract meaningful features and engineer new ones to capture important patterns and relationships in the data, enhancing predictive performance.\n",
    "\n",
    "`Model Training: Implement and fine-tune various machine learning algorithms, including logistic regression, random forest, and gradient boosting, to develop a predictive binary classification model.`\n",
    "\n",
    "`Model Evaluation: Evaluate the performance of the trained models using appropriate metrics such as accuracy, precision, recall, F1 score, and ROC AUC score. Employ cross-validation techniques to ensure robustness and generalization.`\n",
    "\n",
    "Model Deployment: Deploy the best-performing model to production or integrate it into existing systems for real-time prediction of machine failures. Provide recommendations for proactive maintenance actions based on model insights.\n",
    "\n",
    "By achieving these objectives, the notebook aims to empower industrial stakeholders with predictive analytics capabilities, enabling them to anticipate and mitigate potential machine failures, optimize maintenance schedules, and improve overall operational efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e943075b-1094-477e-9b3f-9beadecd33a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, roc_auc_score, f1_score, fbeta_score, recall_score, precision_score\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5f8d7ca5-45f1-40d5-8938-9399690b4dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(r'C:\\Users\\USER\\Documents\\Python Scripts\\Machine learning\\Neural networks\\Predictive Maintenance\\predictive-maintenance-supervised-learning\\scripts')\n",
    "  # Add the directory containing training and evaluation scripts to Python path\n",
    "\n",
    "from model_training import fit_models, tune_and_fit\n",
    "from  model_evaluation import eval_preds,  predict_and_evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cc348df9-7cfb-4cab-a560-d40560e07e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load processed data for model training and evaluation(binary)\n",
    "\n",
    "import joblib\n",
    "\n",
    "# Load resampled data for Machine Failure\n",
    "X_res_mf, y_res_mf = joblib.load(r'C:\\Users\\USER\\Documents\\Python Scripts\\Machine learning\\Neural networks\\Predictive Maintenance\\predictive-maintenance-supervised-learning\\data\\resampled_machine_failure.pkl')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74984273-f388-485e-b571-629fd5c3e203",
   "metadata": {},
   "source": [
    "## Binary task ¶\n",
    "\n",
    "###  Preliminaries \n",
    "\n",
    "The goal of this section is to find the best model for binary classification of the dataset to predict whether or not there will be Machine Failure. Classification algorithms are part of data mining and use supervised machine learning methods to make predictions about data. In particular, a set of data already divided (”labeled”) into two or more classes of belonging is provided as input thanks to which a classification model is created, which will than be used on new (”unlabeled”) data to assign them to the appropriate class. The starting dataset is usually divided into three groups: the training dataset, i.e. the sample of data used to fit the model, the validation dataset, i.e. the sample of data used to provide an evaluation of a model fit on the training dataset while tuning model hyperparameters and the test dataset, which has the purpose of testing the model. At the beginning of a project a data scientist must make this division and the common ratios used are:\n",
    "\n",
    "70% train, 15% val, 15% test.\n",
    "\n",
    "80% train, 10% val, 10% test.\n",
    "\n",
    "60% train, 20% val, 20% test.\n",
    "\n",
    "In this project we use the ratio (80/10/10) for the split because we test the model for all of these strategies and find that it is the best one. The classification techniques we choose to implement are the following:\n",
    "\n",
    "- Logistic Regression: it estimates the probability of a dependent variable as a function of independent variables. The dependent variable is the output that we are trying to predict while the independent variables or explanatory variables are the factors that we feel could influence the output. For its simplicity and interpretability, we decide to use Logistic Regression as a Benchmark model, a basic model that represents the starting point\n",
    "for comparing the results obtained from other models.\n",
    "\n",
    "- K-nearest neighbors (K-NN): algorithm based on the calculation of the distance between the elements of the dataset. Data is assigned to a certain class if close enough to the other data of the same class. Parameter K represents the number of neighboring data taken into account when assigning classes.\n",
    "\n",
    "- Support Vector Machine: its aim is to find a hyperplane in an N-dimensional space (N—the number of features) that distinctly classifies the data points while maximizing the margin distance, i.e. the distance between data points of both classes.\n",
    "Random Forest: it uses ensemble learning, which is a technique that combines many classifiers to provide solutions to complex problems. Random Forest uses bagging technique: it constructs a multitude of decision trees in parallel, all with the same importance, and the output is the class selected by most trees.\n",
    "\n",
    "- XGBoost: is a gradient-boosted decision tree (GBDT) machine learning library. A Gradient Boosting Decision Tree (GBDT) is a decision tree ensemble learning algorithm similar to Random Forest, from which differs because it uses a boosting technique: it iteratively trains an ensemble of shallow decision trees, with each iteration using the error residuals of the previous model to fit the next model. The final prediction is a weighted sum of all of the tree predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "64dc8032-38bc-4a4e-a505-ebf639a1f577",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train-validation-test split\n",
    "X, y = X_res_mf, y_res_mf\n",
    "X_trainval, X_test, y_trainval, y_test = train_test_split(X, y, test_size=0.1, stratify=y, random_state=0)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_trainval, y_trainval, test_size=0.11, stratify=y_trainval, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c262e3f1-22a8-498c-9ef1-ca1f1ba79880",
   "metadata": {},
   "source": [
    "### Feature selection attempts \n",
    "\n",
    "Before going into the training of the models just mentioned we try to perform feature selection, exploiting the considerations  made about the correlation heatmap and the exploratory data analysis: just to remind, it was noticed that the features `\"Process temperature\"` and `\"Air temperature\"` are positively correlated, and `\"Torque\"` and `\"Rotational speed\"` are negatively correlated. From the feature selection we see from the information scores that the product between \"Torque\" and \"Rotational speed\" has more importance than them individually and same can be said about the importance of the product and difference of `\"Process temperature\"` and `\"Air temperature\"` compared to the individual features . For these reasons, completely deleting these columns seems to be a bad choice because important information can be lost but at the same time it is reasonable to see what happens if we combine them taken by pairs. Therefore we proceed to compare the results obtained by fitting the classification models without tuning any parameter on the following datasets:\n",
    "\n",
    "- The original one (without new features)\r",
    "- Tthe one obtained by removing the \"Process temperature\" and \"Air temperature\" columns, replacing them with a column of their produc and differencet\n",
    "- T\n",
    "the one obtained by removing \"Torque\" and \"Rotational speed\", replacing them with a column of their product- Ca comation of ine the previous operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0e7e919c-1c30-4932-b2a0-2840d447cc9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Air temperature [K]</th>\n",
       "      <th>Process temperature [K]</th>\n",
       "      <th>Rotational speed [rpm]</th>\n",
       "      <th>Torque [Nm]</th>\n",
       "      <th>Tool wear [min]</th>\n",
       "      <th>temp_product</th>\n",
       "      <th>temp_difference</th>\n",
       "      <th>power_output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3829</th>\n",
       "      <td>1.198103</td>\n",
       "      <td>0.738714</td>\n",
       "      <td>1.901342</td>\n",
       "      <td>-1.553133</td>\n",
       "      <td>0.755412</td>\n",
       "      <td>1.037842</td>\n",
       "      <td>1.298989</td>\n",
       "      <td>-1.364125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>335</th>\n",
       "      <td>-1.301338</td>\n",
       "      <td>-1.283370</td>\n",
       "      <td>1.171143</td>\n",
       "      <td>-1.412659</td>\n",
       "      <td>-1.569952</td>\n",
       "      <td>-1.332262</td>\n",
       "      <td>-0.698269</td>\n",
       "      <td>-1.438805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>-0.951417</td>\n",
       "      <td>-1.215968</td>\n",
       "      <td>-1.097490</td>\n",
       "      <td>0.814846</td>\n",
       "      <td>-0.847204</td>\n",
       "      <td>-1.094564</td>\n",
       "      <td>-0.099092</td>\n",
       "      <td>0.450633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3757</th>\n",
       "      <td>1.148114</td>\n",
       "      <td>0.738714</td>\n",
       "      <td>-0.924695</td>\n",
       "      <td>1.306503</td>\n",
       "      <td>1.148210</td>\n",
       "      <td>1.007718</td>\n",
       "      <td>1.199126</td>\n",
       "      <td>1.257284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10002</th>\n",
       "      <td>1.318212</td>\n",
       "      <td>0.603908</td>\n",
       "      <td>-1.145407</td>\n",
       "      <td>1.133277</td>\n",
       "      <td>-1.097286</td>\n",
       "      <td>1.051612</td>\n",
       "      <td>1.738657</td>\n",
       "      <td>0.821230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10013</th>\n",
       "      <td>0.362873</td>\n",
       "      <td>0.264788</td>\n",
       "      <td>0.329172</td>\n",
       "      <td>-0.714577</td>\n",
       "      <td>1.658028</td>\n",
       "      <td>0.329803</td>\n",
       "      <td>0.332607</td>\n",
       "      <td>-0.755922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6922</th>\n",
       "      <td>0.348293</td>\n",
       "      <td>0.873520</td>\n",
       "      <td>-0.289255</td>\n",
       "      <td>-0.017960</td>\n",
       "      <td>-0.910051</td>\n",
       "      <td>0.583972</td>\n",
       "      <td>-0.598406</td>\n",
       "      <td>-0.076181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1737</th>\n",
       "      <td>-0.751461</td>\n",
       "      <td>-1.418176</td>\n",
       "      <td>0.563573</td>\n",
       "      <td>-0.690225</td>\n",
       "      <td>0.598293</td>\n",
       "      <td>-1.061903</td>\n",
       "      <td>0.599948</td>\n",
       "      <td>-0.556964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2591</th>\n",
       "      <td>-0.501517</td>\n",
       "      <td>-0.878954</td>\n",
       "      <td>-0.501068</td>\n",
       "      <td>-0.108264</td>\n",
       "      <td>-1.412833</td>\n",
       "      <td>-0.681217</td>\n",
       "      <td>0.300360</td>\n",
       "      <td>-0.352612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4594</th>\n",
       "      <td>1.448047</td>\n",
       "      <td>0.671311</td>\n",
       "      <td>1.566900</td>\n",
       "      <td>-1.543099</td>\n",
       "      <td>0.315478</td>\n",
       "      <td>1.159131</td>\n",
       "      <td>1.898166</td>\n",
       "      <td>-1.490539</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8206 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Air temperature [K]  Process temperature [K]  Rotational speed [rpm]  \\\n",
       "3829              1.198103                 0.738714                1.901342   \n",
       "335              -1.301338                -1.283370                1.171143   \n",
       "182              -0.951417                -1.215968               -1.097490   \n",
       "3757              1.148114                 0.738714               -0.924695   \n",
       "10002             1.318212                 0.603908               -1.145407   \n",
       "...                    ...                      ...                     ...   \n",
       "10013             0.362873                 0.264788                0.329172   \n",
       "6922              0.348293                 0.873520               -0.289255   \n",
       "1737             -0.751461                -1.418176                0.563573   \n",
       "2591             -0.501517                -0.878954               -0.501068   \n",
       "4594              1.448047                 0.671311                1.566900   \n",
       "\n",
       "       Torque [Nm]  Tool wear [min]  temp_product  temp_difference  \\\n",
       "3829     -1.553133         0.755412      1.037842         1.298989   \n",
       "335      -1.412659        -1.569952     -1.332262        -0.698269   \n",
       "182       0.814846        -0.847204     -1.094564        -0.099092   \n",
       "3757      1.306503         1.148210      1.007718         1.199126   \n",
       "10002     1.133277        -1.097286      1.051612         1.738657   \n",
       "...            ...              ...           ...              ...   \n",
       "10013    -0.714577         1.658028      0.329803         0.332607   \n",
       "6922     -0.017960        -0.910051      0.583972        -0.598406   \n",
       "1737     -0.690225         0.598293     -1.061903         0.599948   \n",
       "2591     -0.108264        -1.412833     -0.681217         0.300360   \n",
       "4594     -1.543099         0.315478      1.159131         1.898166   \n",
       "\n",
       "       power_output  \n",
       "3829      -1.364125  \n",
       "335       -1.438805  \n",
       "182        0.450633  \n",
       "3757       1.257284  \n",
       "10002      0.821230  \n",
       "...             ...  \n",
       "10013     -0.755922  \n",
       "6922      -0.076181  \n",
       "1737      -0.556964  \n",
       "2591      -0.352612  \n",
       "4594      -1.490539  \n",
       "\n",
       "[8206 rows x 8 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "56d3b592-e522-4e7c-a7b1-e2ee470f4437",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Target'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\envs\\certisims\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3791\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3790\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3791\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3792\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mindex.pyx:152\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mindex.pyx:160\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\index_class_helper.pxi:70\u001b[0m, in \u001b[0;36mpandas._libs.index.Int64Engine._check_type\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Target'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 14\u001b[0m\n\u001b[0;32m     12\u001b[0m XX_train \u001b[38;5;241m=\u001b[39m X_train\u001b[38;5;241m.\u001b[39mdrop(columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtemp_product\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtemp_difference\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpower_output\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     13\u001b[0m XX_val \u001b[38;5;241m=\u001b[39m X_val\u001b[38;5;241m.\u001b[39mdrop(columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtemp_product\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtemp_difference\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpower_output\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m---> 14\u001b[0m metrics_0 \u001b[38;5;241m=\u001b[39m \u001b[43mfit_models\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclf\u001b[49m\u001b[43m,\u001b[49m\u001b[43mclf_str\u001b[49m\u001b[43m,\u001b[49m\u001b[43mXX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43mXX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43my_val\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# Fit on temperature product and differenc train\u001b[39;00m\n\u001b[0;32m     17\u001b[0m XX_train \u001b[38;5;241m=\u001b[39m X_train\u001b[38;5;241m.\u001b[39mdrop(columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mProcess temperature\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAir temperature\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpower_output\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[1;32m~\\Documents\\Python Scripts\\Machine learning\\Neural networks\\Predictive Maintenance\\predictive-maintenance-supervised-learning\\scripts\\model_training.py:32\u001b[0m, in \u001b[0;36mfit_models\u001b[1;34m(clf, clf_str, X_train, X_val, y_train, y_val)\u001b[0m\n\u001b[0;32m     30\u001b[0m metrics \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(columns\u001b[38;5;241m=\u001b[39mclf_str)\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m model, model_name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(clf, clf_str):\n\u001b[1;32m---> 32\u001b[0m     model\u001b[38;5;241m.\u001b[39mfit(X_train, \u001b[43my_train\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mTarget\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m)\n\u001b[0;32m     33\u001b[0m     y_val_pred \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X_val)\n\u001b[0;32m     34\u001b[0m     metrics[model_name] \u001b[38;5;241m=\u001b[39m eval_preds(model, X_val, y_val, y_val_pred, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\certisims\\lib\\site-packages\\pandas\\core\\series.py:1040\u001b[0m, in \u001b[0;36mSeries.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1037\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[key]\n\u001b[0;32m   1039\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m key_is_scalar:\n\u001b[1;32m-> 1040\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1042\u001b[0m \u001b[38;5;66;03m# Convert generator to list before going through hashable part\u001b[39;00m\n\u001b[0;32m   1043\u001b[0m \u001b[38;5;66;03m# (We will iterate through the generator there to check for slices)\u001b[39;00m\n\u001b[0;32m   1044\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\certisims\\lib\\site-packages\\pandas\\core\\series.py:1156\u001b[0m, in \u001b[0;36mSeries._get_value\u001b[1;34m(self, label, takeable)\u001b[0m\n\u001b[0;32m   1153\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[label]\n\u001b[0;32m   1155\u001b[0m \u001b[38;5;66;03m# Similar to Index.get_value, but we do not fall back to positional\u001b[39;00m\n\u001b[1;32m-> 1156\u001b[0m loc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1158\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(loc):\n\u001b[0;32m   1159\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[loc]\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\certisims\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3798\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3793\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   3794\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[0;32m   3795\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[0;32m   3796\u001b[0m     ):\n\u001b[0;32m   3797\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[1;32m-> 3798\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3799\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3800\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3801\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3802\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3803\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Target'"
     ]
    }
   ],
   "source": [
    "# Models\n",
    "lr = LogisticRegression()\n",
    "knn = KNeighborsClassifier()\n",
    "svc = SVC(probability=True)\n",
    "rfc = RandomForestClassifier()\n",
    "xgb = XGBClassifier() \n",
    "\n",
    "clf = [lr,knn,svc,rfc,xgb]\n",
    "clf_str = ['LR','KNN','SVC','RFC','XGB'] \n",
    "\n",
    "# Fit on raw train\n",
    "XX_train = X_train.drop(columns=['temp_product','temp_difference','power_output'])\n",
    "XX_val = X_val.drop(columns=['temp_product','temp_difference','power_output'])\n",
    "metrics_0 = fit_models(clf,clf_str,XX_train,XX_val,y_train,y_val)\n",
    "\n",
    "# Fit on temperature product and difference train\n",
    "XX_train = X_train.drop(columns=['Process temperature','Air temperature','power_output'])\n",
    "XX_val = X_val.drop(columns=['Process temperature','Air temperature','power_output'])\n",
    "metrics_1 = fit_models(clf,clf_str,XX_train,XX_val,y_train,y_val)\n",
    "\n",
    "# Fit on power product train\n",
    "XX_train = X_train.drop(columns=['Rotational speed','Torque','temp_product','temp_difference'])\n",
    "XX_val = X_val.drop(columns=['Rotational speed','Torque','temp_product','temp_difference'])\n",
    "metrics_2 = fit_models(clf,clf_str,XX_train,XX_val,y_train,y_val)\n",
    "\n",
    "# Fit on both products train\n",
    "XX_train = X_train.drop(columns=['Process temperature','Air temperature','Rotational speed','Torque'])\n",
    "XX_val = X_val.drop(columns=['Process temperature','Air temperature','Rotational speed','Torque'])\n",
    "metrics_3 = fit_models(clf,clf_str,XX_train,XX_val,y_train,y_val)\n",
    "\n",
    "# classification metrics barplot\n",
    "fig, axs = plt.subplots(nrows=2, ncols=3, figsize=(18,8))\n",
    "fig.suptitle('Classification metrics')\n",
    "for j, model in enumerate(clf_str):\n",
    "    ax = axs[j//3,j-3*(j//3)]\n",
    "    model_metrics = pd.DataFrame(data=[metrics_0[model],metrics_1[model],metrics_2[model],metrics_3[model]])\n",
    "    model_metrics.index = ['Original','Temperature','Power','Both']\n",
    "    model_metrics.transpose().plot(ax=ax, kind='bar', rot=0, )\n",
    "    ax.title.set_text(model)\n",
    "    ax.get_legend().remove()\n",
    "fig.subplots_adjust(top=0.9, left=0.1, right=0.9, bottom=0.12)\n",
    "axs.flatten()[-2].legend(title='Dataset', loc='upper center',\n",
    "                         bbox_to_anchor=(0.5, -0.12), ncol=4, fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a811c8-6369-4a41-b359-ae44923e92a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
